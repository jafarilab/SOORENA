#!/usr/bin/env python3
"""
Predict on the ~250K unlabeled papers that were NOT used during Stage 1 training.

This script uses the unlabeled_unused_for_prediction.csv file generated by
extract_training_samples.py to ensure we only predict on papers that the
model has never seen during training.
"""
import sys
from pathlib import Path

# Add repository root to Python path
REPO_ROOT = Path(__file__).resolve().parent.parent.parent.parent
sys.path.insert(0, str(REPO_ROOT))

import pandas as pd
from tqdm import tqdm
from scripts.python.prediction.predict import MechanismPredictor
import os
import argparse


def main():
    """Predict on unused unlabeled papers."""

    parser = argparse.ArgumentParser(description='Predict on unused unlabeled papers')
    parser.add_argument('--input', default='data/processed/unlabeled_unused_for_prediction.csv',
                        help='Input CSV with unused unlabeled papers')
    parser.add_argument('--output', default='results/unused_unlabeled_predictions.csv',
                        help='Output CSV for predictions')
    parser.add_argument('--checkpoint-interval', type=int, default=10000,
                        help='Save checkpoint every N predictions')
    args = parser.parse_args()

    print("=" * 80)
    print("PREDICT ON UNUSED UNLABELED PAPERS")
    print("=" * 80)
    print()

    # Create results directory
    os.makedirs('results', exist_ok=True)

    # Load unused unlabeled papers
    print(f"Loading unused unlabeled papers from: {args.input}")
    if not os.path.exists(args.input):
        print(f"ERROR: Input file not found!")
        print(f"Please run: python scripts/python/data_processing/extract_training_samples.py")
        return

    unused_df = pd.read_csv(args.input)
    print(f"   Loaded {len(unused_df):,} papers to predict on")
    print()

    # Check for existing checkpoint
    checkpoint_file = args.output.replace('.csv', '_checkpoint.csv')
    if os.path.exists(checkpoint_file):
        print(f"Found checkpoint file: {checkpoint_file}")
        existing_df = pd.read_csv(checkpoint_file)
        already_predicted = set(existing_df['PMID'])
        unused_df = unused_df[~unused_df['PMID'].isin(already_predicted)]
        print(f"   Already predicted: {len(already_predicted):,}")
        print(f"   Remaining: {len(unused_df):,}")
        print()
        results = existing_df.to_dict('records')
    else:
        results = []

    if len(unused_df) == 0:
        print("All papers already predicted! Loading final results...")
        final_df = pd.read_csv(checkpoint_file)
        final_df.to_csv(args.output, index=False)
        print(f"✓ Saved to: {args.output}")
        return

    # Initialize predictor
    print("Initializing predictor (loading models)...")
    predictor = MechanismPredictor()
    print("✓ Models loaded")
    print()

    # Predict with checkpointing
    print(f"Starting predictions (checkpoint every {args.checkpoint_interval:,} papers)...")
    print()

    for idx, row in tqdm(unused_df.iterrows(), total=len(unused_df), desc="Predicting"):
        pred = predictor.predict(row['text'], '')
        results.append({
            'PMID': row['PMID'],
            'has_mechanism': pred['has_mechanism'],
            'stage1_confidence': pred['stage1_confidence'],
            'mechanism_type': pred['mechanism_type'] if pred['mechanism_type'] else 'none',
            'stage2_confidence': pred['stage2_confidence'] if pred['stage2_confidence'] else 0.0
        })

        # Save checkpoint
        if len(results) % args.checkpoint_interval == 0:
            pd.DataFrame(results).to_csv(checkpoint_file, index=False)
            print(f"\n✓ Checkpoint saved: {len(results):,} predictions")

    # Save final predictions
    results_df = pd.DataFrame(results)
    results_df.to_csv(args.output, index=False)

    # Remove checkpoint
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)

    print()
    print("=" * 80)
    print("PREDICTION COMPLETE")
    print("=" * 80)
    print(f"Saved to: {args.output}")
    print(f"Total predictions: {len(results_df):,}")
    print()
    print(f"Papers with mechanisms:    {results_df['has_mechanism'].sum():,}")
    print(f"Papers without mechanisms: {(~results_df['has_mechanism']).sum():,}")
    print()

    # Show mechanism type breakdown
    if results_df['has_mechanism'].sum() > 0:
        print("Mechanism type distribution:")
        mech_counts = results_df[results_df['has_mechanism']]['mechanism_type'].value_counts()
        for mech_type, count in mech_counts.items():
            print(f"  {mech_type:20s}: {count:,}")

    print()
    print("Next step: Merge all data for Shiny app")
    print("  python scripts/python/data_processing/merge_final_shiny_data.py")
    print("=" * 80)


if __name__ == "__main__":
    main()
